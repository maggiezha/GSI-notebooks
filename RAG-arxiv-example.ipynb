{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c617f2c6-a1f6-45ef-bfc1-85d1f1aecb51",
   "metadata": {},
   "source": [
    "This notebook shows how to form a simple pipeline of retrieval augmented generation (RAG) using Langchain and three models, to retrieve information from research papers in Arxiv and answer questions related to the research papers. \n",
    "\n",
    "Firstly, we use keywords to search for research papers in Arxiv, then we use the models including a embedding model (NV-embed-QA), a reranking model (nv-rerankqa-mistral-4b-v3), and a large language model (meta/llama-3.1-8b-instruct) to make a simple RAG, which can retrieve answers from the research papers based on the questions we ask.\n",
    "\n",
    "You don't need to download the models, as the LLM / embedding / reranking models are hosted at NVIDIA endpoints (https://build.nvidia.com/). You need to generate a NVIDIA API key to use the model endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301ddc09-ae62-4596-a2b6-42b4b4678e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet pip arxiv pymupdf  pypdf langchain langchain-nvidia-ai-endpoints  langchain-community faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5f013-a924-4333-914f-ef44c0b89994",
   "metadata": {},
   "source": [
    "Firstly, We can use Langchain's ArxivLoader to get the basic information of the research paper, such as the title, authors, abstract, published date, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d200fd-ea3a-48a3-aae0-e12e8863fb4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4025699c-e234-42a8-8cf4-5e2de7a2fcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2024-06-19',\n",
       " 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation',\n",
       " 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen',\n",
       " 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata  # meta-information of the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd428c9-36e2-4e9b-9f6d-2add290f1268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R2AG: Incorporating Retrieval Information into Retrieval Augmented\\nGeneration\\nFuda Ye1, Shuangyin Li1,*, Yongqi Zhang2, Lei Chen2,3\\n1South China Normal University\\n2The Hong Kong University of Science and Technology (Guangzhou)\\n3The Hong Kong University of Science and Technology\\nfudayip@m.scnu.edu.cn, shuangyinli@scnu.edu.cn, yongqizhang@hkust-gz.edu.cn, leichen@cse.ust.hk\\nAbstract\\nRetrieval augmented generation (RAG) has\\nbeen applied in many scenarios to augment\\nlarge language models (LLMs) with external\\ndocuments provided by retrievers. However,\\na semantic gap exists between LLMs and\\nretrievers due to differences in their training\\nobjectives and architectures. This misalign-\\nment forces LLMs to passively accept the\\ndocuments provided by the retrievers, leading\\nto incomprehension in the generation process,\\nwhere the LLMs are burdened with the task of\\ndistinguishing these documents using their in-\\nherent knowledge. This paper proposes R2AG,\\na novel enhanced RAG framework to fill this\\ngap by incorporating Retrieval information into\\nRetrieval Augmented Generation. Specifically,\\nR2AG utilizes the nuanced features from the\\nretrievers and employs a R2-Former to capture\\nretrieval information. Then, a retrieval-aware\\nprompting strategy is designed to integrate re-\\ntrieval information into LLMs’ generation. No-\\ntably, R2AG suits low-source scenarios where\\nLLMs and retrievers are frozen. Extensive ex-\\nperiments across five datasets validate the effec-\\ntiveness, robustness, and efficiency of R2AG.\\nOur analysis reveals that retrieval information\\nserves as an anchor to aid LLMs in the gener-\\nation process, thereby filling the semantic gap.\\n1\\nIntroduction\\nRetrieval augmented generation (RAG) (Lewis\\net al., 2020) significantly enhances the capabilities\\nof large language models (LLMs) by integrating\\nexternal, non-parametric knowledge provided by\\nretrievers. In RAG framework, the retriever lo-\\ncates and looks up useful documents based on a\\ngiven query, and then the LLM interacts with these\\nretrieved results to generate a response. The coordi-\\nnation of retrieval and generation achieves impres-\\nsive performance without additional training. Espe-\\ncially in domain-specific and knowledge-intensive\\n*Corresponding author. The source code is available at\\nhttps://github.com/yefd/RRAG.git.\\nRAG\\nR2AG\\nRetriever\\nLLM\\nCombine\\nQuery &\\nDocuments\\nQuery\\nTop-k\\nDocuments\\nR2-Former\\n/\\nRetriever\\nLLM\\nCombine\\nQuery &\\nDocuments\\nQuery\\nTop-k\\nDocuments\\nRetrieval-aware\\nPrompting\\nSemantic Gap\\nFigure 1: A comparison between RAG and R2AG.\\nR2AG employs a trainable R2-Former to bridge the\\nsemantic gap between retrievers and LLMs. Optionally,\\nLLMs can be fine-tuned to understand the retrieval in-\\nformation further.\\ntasks, RAG offers real-time knowledge with high\\ninterpretability to LLMs, effectively mitigating the\\nhallucination problem (Mallen et al., 2023).\\nHowever, there exists a semantic gap between re-\\ntrievers and LLMs due to their vastly different train-\\ning objectives and architectures (Behna'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:3000]  # a content of the Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3480f-7a72-4f0a-871e-a655e2bb800e",
   "metadata": {},
   "source": [
    "We can also use Langchain's ArxivRetriever to retrieve multiple papers from Arxiv based on the keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48294229-fd76-44bb-8793-1c389aa07bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/2406.13249v1', 'Published': datetime.date(2024, 6, 19), 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation', 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen'}, page_content=\"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\"),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2202.01110v2', 'Published': datetime.date(2022, 2, 13), 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu'}, page_content='Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2407.03955v1', 'Published': datetime.date(2024, 7, 4), 'Title': 'Meta-prompting Optimized Retrieval-augmented Generation', 'Authors': 'João Rodrigues, António Branco'}, page_content='Retrieval-augmented generation resorts to content retrieved from external\\nsources in order to leverage the performance of large language models in\\ndownstream tasks. The excessive volume of retrieved content, the possible\\ndispersion of its parts, or their out of focus range may happen nevertheless to\\neventually have a detrimental rather than an incremental effect. To mitigate\\nthis issue and improve retrieval-augmented generation, we propose a method to\\nrefine the retrieved content before it is included in the prompt by resorting\\nto meta-prompting optimization. Put to empirical test with the demanding\\nmulti-hop question answering task from the StrategyQA dataset, the evaluation\\nresults indicate that this method outperforms a similar retrieval-augmented\\nsystem but without this method by over 30%.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import ArxivRetriever\n",
    "retriever = ArxivRetriever(load_max_docs=5)\n",
    "docs = retriever.invoke(\n",
    "    \"Retrieval Augmented Generation\",\n",
    ")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa81047-29b9-4996-86a4-b69fd135fbc1",
   "metadata": {},
   "source": [
    "Or retrieve the research papers based on the author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006ae06e-906b-410e-9c38-165a3019f5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/1507.08396v1', 'Published': datetime.date(2015, 7, 30), 'Title': 'Tag-Weighted Topic Model For Large-scale Semi-Structured Documents', 'Authors': 'Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, Rong Pan'}, page_content='To date, there have been massive Semi-Structured Documents (SSDs) during the\\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\\nthe unstructured text, and recently, some other methods have been proposed to\\nmodel the unstructured text with specific tags. To build a general model for\\nSSDs remains an important problem in terms of both model fitness and\\nefficiency. We propose a novel method to model the SSDs by a so-called\\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\\ntags and words information, not only to learn the document-topic and topic-word\\ndistributions, but also to infer the tag-topic distributions for text mining\\ntasks. We present an efficient variational inference method with an EM\\nalgorithm for estimating the model parameters. Meanwhile, we propose three\\nlarge-scale solutions for our model under the MapReduce distributed computing\\nplatform for modeling large-scale SSDs. The experimental results show the\\neffectiveness, efficiency and the robustness by comparing our model with the\\nstate-of-the-art methods in document modeling, tags prediction and text\\nclassification. We also show the performance of the three distributed solutions\\nin terms of time and accuracy on document modeling.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2401.03426v1', 'Published': datetime.date(2024, 1, 7), 'Title': 'On Leveraging Large Language Models for Enhancing Entity Resolution', 'Authors': 'Huahang Li, Longyu Feng, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song, Lei Chen'}, page_content='Entity resolution, the task of identifying and consolidating records that\\npertain to the same real-world entity, plays a pivotal role in various sectors\\nsuch as e-commerce, healthcare, and law enforcement. The emergence of Large\\nLanguage Models (LLMs) like GPT-4 has introduced a new dimension to this task,\\nleveraging their advanced linguistic capabilities. This paper explores the\\npotential of LLMs in the entity resolution process, shedding light on both\\ntheir advantages and the computational complexities associated with large-scale\\nmatching. We introduce strategies for the efficient utilization of LLMs,\\nincluding the selection of an optimal set of matching questions, namely MQsSP,\\nwhich is proved to be a NP-hard problem. Our approach optimally chooses the\\nmost effective matching questions while keep consumption limited to your budget\\n. Additionally, we propose a method to adjust the distribution of possible\\npartitions after receiving responses from LLMs, with the goal of reducing the\\nuncertainty of entity resolution. We evaluate the effectiveness of our approach\\nusing entropy as a metric, and our experimental results demonstrate the\\nefficiency and effectiveness of our proposed methods, offering promising\\nprospects for real-world applications.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2403.06434v1', 'Published': datetime.date(2024, 3, 11), 'Title': 'BoostER: Leveraging Large Language Models for Enhancing Entity Resolution', 'Authors': 'Huahang Li, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song, Lei Chen'}, page_content='Entity resolution, which involves identifying and merging records that refer\\nto the same real-world entity, is a crucial task in areas like Web data\\nintegration. This importance is underscored by the presence of numerous\\nduplicated and multi-version data resources on the Web. However, achieving\\nhigh-quality entity resolution typically demands significant effort. The advent\\nof Large Language Models (LLMs) like GPT-4 has demonstrated advanced linguistic\\ncapabilities, which can be a new paradigm for this task. In this paper, we\\npropose a demonstration system named BoostER that examines the possibility of\\nleveraging LLMs in the entity resolution process, revealing advantages in both\\neasy deployment and low cost. Our approach optimally selects a set of matching\\nquestions and poses them to LLMs for verification, then refines the\\ndistribution of entity resolution results with the response of LLMs. This\\noffers promising prospects to achieve a high-quality entity resolution result\\nfor real-world applications, especially to individuals or small companies\\nwithout the need for extensive model training or significant financial\\ninvestment.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\n",
    "    \"Shuangyin Li\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5616b43-0aa8-4d9a-86ce-0d0a4dbad57f",
   "metadata": {},
   "source": [
    "You can choose any paper to learn more information, Langchain provides PyPDFLoader for PDF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84386c9d-b66f-4efe-8dd4-ec718c7f711c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://arxiv.org/pdf/2005.11401', 'page': 0}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research;‡University College London;⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [ 47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [ 51,52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4  [cs.CL]  12 Apr 2021')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# You can replace the below link with a different link to a PDF file \n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2005.11401\")\n",
    "pages = loader.load_and_split()\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364d275-8525-4e04-b0be-4623a3dfcd54",
   "metadata": {},
   "source": [
    "A simple example of embedding model: choose any sentence from above page that we just printed out to see its embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df20c5da-591c-4a58-a339-a2e500fcc848",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize LLM\n",
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# NVIDIA AI Foundation Endpoints\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"your API key starting with nvapi-\" \n",
    "\n",
    "llm = ChatNVIDIA(\n",
    "  model=\"meta/llama-3.1-8b-instruct\",\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b04d77-eee3-4efc-9fce-8650058f883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can replace the example text with any text you want to try.\n",
    "example_text = \"Large pre-trained language models have been shown to store factual knowledge\\nin their parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b860a698-961c-4919-8922-8e3be26c1bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02069091796875,\n",
       " -0.0182342529296875,\n",
       " 0.01340484619140625,\n",
       " 8.600950241088867e-05,\n",
       " 0.0223236083984375,\n",
       " 0.050994873046875,\n",
       " 0.0338134765625,\n",
       " -0.01369476318359375,\n",
       " -0.0094451904296875,\n",
       " -0.043426513671875,\n",
       " 0.01171112060546875,\n",
       " 0.02374267578125,\n",
       " 0.0107879638671875,\n",
       " -0.025299072265625,\n",
       " -0.004131317138671875,\n",
       " 0.00812530517578125,\n",
       " -0.00879669189453125,\n",
       " 0.0154571533203125,\n",
       " -0.012420654296875,\n",
       " 0.01025390625,\n",
       " 0.005096435546875,\n",
       " 0.045654296875,\n",
       " 0.01207733154296875,\n",
       " -0.0262908935546875,\n",
       " 0.018218994140625,\n",
       " 0.01947021484375,\n",
       " 0.011444091796875,\n",
       " -0.021240234375,\n",
       " 0.0096282958984375,\n",
       " 0.06756591796875,\n",
       " 0.051849365234375,\n",
       " -0.01100921630859375,\n",
       " 0.03155517578125,\n",
       " 0.05328369140625,\n",
       " 0.002727508544921875,\n",
       " 0.034454345703125,\n",
       " 0.00989532470703125,\n",
       " -0.0606689453125,\n",
       " -0.01922607421875,\n",
       " 0.032135009765625,\n",
       " -0.0113372802734375,\n",
       " -0.0289306640625,\n",
       " 0.0079193115234375,\n",
       " 0.07177734375,\n",
       " -0.03973388671875,\n",
       " 0.0247039794921875,\n",
       " -0.0963134765625,\n",
       " -0.013763427734375,\n",
       " -0.016845703125,\n",
       " -0.0085906982421875,\n",
       " 0.0294036865234375,\n",
       " 0.011566162109375,\n",
       " 0.05279541015625,\n",
       " 0.051666259765625,\n",
       " -0.01171875,\n",
       " -0.0081329345703125,\n",
       " 0.046966552734375,\n",
       " 0.06304931640625,\n",
       " -0.0016622543334960938,\n",
       " -0.05792236328125,\n",
       " 0.00661468505859375,\n",
       " -0.03411865234375,\n",
       " 0.037689208984375,\n",
       " -0.0200653076171875,\n",
       " 0.0123138427734375,\n",
       " 0.015960693359375,\n",
       " 0.0257568359375,\n",
       " -0.03070068359375,\n",
       " -0.002223968505859375,\n",
       " -0.006168365478515625,\n",
       " 0.047760009765625,\n",
       " -0.0228271484375,\n",
       " 0.0107879638671875,\n",
       " 0.054290771484375,\n",
       " 0.049560546875,\n",
       " 0.00972747802734375,\n",
       " -0.003131866455078125,\n",
       " 0.01861572265625,\n",
       " 0.005832672119140625,\n",
       " 0.02264404296875,\n",
       " -0.04034423828125,\n",
       " -0.004009246826171875,\n",
       " -0.0196685791015625,\n",
       " -0.0012826919555664062,\n",
       " -0.00974273681640625,\n",
       " 0.004558563232421875,\n",
       " 0.00757598876953125,\n",
       " 0.03387451171875,\n",
       " 0.033843994140625,\n",
       " 0.0208587646484375,\n",
       " -0.02056884765625,\n",
       " -0.0078887939453125,\n",
       " 0.01526641845703125,\n",
       " -0.0255889892578125,\n",
       " -0.0253753662109375,\n",
       " -0.0108489990234375,\n",
       " 0.0029754638671875,\n",
       " 0.01068115234375,\n",
       " 0.033935546875,\n",
       " -0.0203704833984375,\n",
       " -0.03656005859375,\n",
       " -0.037445068359375,\n",
       " -0.003143310546875,\n",
       " -0.0109100341796875,\n",
       " -0.0438232421875,\n",
       " -0.043243408203125,\n",
       " 0.003444671630859375,\n",
       " -0.035064697265625,\n",
       " -0.0201263427734375,\n",
       " 0.007720947265625,\n",
       " -0.026092529296875,\n",
       " -0.0164642333984375,\n",
       " -0.045440673828125,\n",
       " 0.01213836669921875,\n",
       " 0.029693603515625,\n",
       " -0.0235443115234375,\n",
       " 0.010223388671875,\n",
       " -0.04815673828125,\n",
       " 0.04437255859375,\n",
       " 0.053375244140625,\n",
       " -0.006496429443359375,\n",
       " -0.02813720703125,\n",
       " 0.025177001953125,\n",
       " -0.00395965576171875,\n",
       " -0.01116180419921875,\n",
       " -0.0017690658569335938,\n",
       " -0.008941650390625,\n",
       " -0.0284423828125,\n",
       " -0.0518798828125,\n",
       " 0.00931549072265625,\n",
       " 0.01019287109375,\n",
       " -0.044525146484375,\n",
       " 0.06951904296875,\n",
       " 0.00815582275390625,\n",
       " 0.032257080078125,\n",
       " 0.04736328125,\n",
       " -0.0305938720703125,\n",
       " -0.0399169921875,\n",
       " 0.00852203369140625,\n",
       " -0.0210723876953125,\n",
       " 0.011627197265625,\n",
       " -0.0657958984375,\n",
       " 0.006137847900390625,\n",
       " 0.0215911865234375,\n",
       " 0.00047397613525390625,\n",
       " 0.005859375,\n",
       " 0.049713134765625,\n",
       " 0.05035400390625,\n",
       " -0.037841796875,\n",
       " -0.00202178955078125,\n",
       " -0.03497314453125,\n",
       " 0.00495147705078125,\n",
       " -0.019866943359375,\n",
       " 0.061187744140625,\n",
       " -0.07879638671875,\n",
       " -0.048004150390625,\n",
       " 0.0172882080078125,\n",
       " -0.002559661865234375,\n",
       " -0.061798095703125,\n",
       " -0.00012862682342529297,\n",
       " -0.0193939208984375,\n",
       " -0.0172882080078125,\n",
       " 0.02032470703125,\n",
       " -0.0133209228515625,\n",
       " 0.0262908935546875,\n",
       " -0.03045654296875,\n",
       " 0.03936767578125,\n",
       " 0.02593994140625,\n",
       " 0.0283050537109375,\n",
       " 0.0352783203125,\n",
       " 0.034423828125,\n",
       " -0.033172607421875,\n",
       " 0.028961181640625,\n",
       " 0.01381683349609375,\n",
       " 0.032012939453125,\n",
       " 0.01214599609375,\n",
       " 0.003231048583984375,\n",
       " -0.0077667236328125,\n",
       " 0.072265625,\n",
       " 0.0046539306640625,\n",
       " -0.0180206298828125,\n",
       " -0.0238494873046875,\n",
       " -0.0207977294921875,\n",
       " 0.05841064453125,\n",
       " -3.6656856536865234e-05,\n",
       " 0.0242462158203125,\n",
       " -0.005771636962890625,\n",
       " -0.00162506103515625,\n",
       " 0.046600341796875,\n",
       " -0.058990478515625,\n",
       " 0.00519561767578125,\n",
       " -0.01233673095703125,\n",
       " 0.038055419921875,\n",
       " -0.056915283203125,\n",
       " -0.0116729736328125,\n",
       " 0.04119873046875,\n",
       " -0.0428466796875,\n",
       " -0.00974273681640625,\n",
       " -0.019378662109375,\n",
       " -0.0015583038330078125,\n",
       " -0.053619384765625,\n",
       " 0.00507354736328125,\n",
       " 0.06524658203125,\n",
       " -0.0137481689453125,\n",
       " 0.01555633544921875,\n",
       " -0.015594482421875,\n",
       " 0.0078277587890625,\n",
       " -0.02166748046875,\n",
       " 0.0269012451171875,\n",
       " 0.042449951171875,\n",
       " -0.055206298828125,\n",
       " 0.0016736984252929688,\n",
       " -0.01409149169921875,\n",
       " 0.0177459716796875,\n",
       " -0.0290069580078125,\n",
       " -0.06060791015625,\n",
       " 0.023773193359375,\n",
       " 0.0151519775390625,\n",
       " -0.0225372314453125,\n",
       " 0.0160369873046875,\n",
       " -0.0164794921875,\n",
       " 0.02056884765625,\n",
       " 0.03533935546875,\n",
       " 0.00897216796875,\n",
       " 0.007221221923828125,\n",
       " 0.0106964111328125,\n",
       " -0.00341033935546875,\n",
       " 0.0626220703125,\n",
       " 0.01216888427734375,\n",
       " 0.037933349609375,\n",
       " -0.031036376953125,\n",
       " -0.05938720703125,\n",
       " 0.03106689453125,\n",
       " 0.032073974609375,\n",
       " 0.03204345703125,\n",
       " -0.0279083251953125,\n",
       " 0.0151519775390625,\n",
       " 0.02679443359375,\n",
       " 0.03778076171875,\n",
       " 0.0036754608154296875,\n",
       " 0.01397705078125,\n",
       " -0.0209503173828125,\n",
       " 0.024444580078125,\n",
       " 0.006870269775390625,\n",
       " 0.0161285400390625,\n",
       " -0.02313232421875,\n",
       " 0.02734375,\n",
       " 0.01898193359375,\n",
       " 0.0181884765625,\n",
       " 0.017181396484375,\n",
       " 0.040435791015625,\n",
       " -0.0254058837890625,\n",
       " 0.06201171875,\n",
       " -0.037506103515625,\n",
       " -0.005992889404296875,\n",
       " -0.028045654296875,\n",
       " 0.008514404296875,\n",
       " -0.0180206298828125,\n",
       " 0.0113983154296875,\n",
       " 0.0195465087890625,\n",
       " -0.0241241455078125,\n",
       " -0.00737762451171875,\n",
       " -0.02520751953125,\n",
       " -0.0244293212890625,\n",
       " 0.0248870849609375,\n",
       " 0.0295562744140625,\n",
       " -0.01611328125,\n",
       " -0.006626129150390625,\n",
       " 0.07257080078125,\n",
       " -0.029449462890625,\n",
       " -0.0005431175231933594,\n",
       " 0.01041412353515625,\n",
       " -0.032257080078125,\n",
       " -0.0188140869140625,\n",
       " 0.05328369140625,\n",
       " 0.037506103515625,\n",
       " -0.03985595703125,\n",
       " 0.00136566162109375,\n",
       " -0.0015249252319335938,\n",
       " -0.00885772705078125,\n",
       " -0.0162200927734375,\n",
       " 0.038238525390625,\n",
       " 0.024688720703125,\n",
       " -0.0256805419921875,\n",
       " -0.05609130859375,\n",
       " 0.045654296875,\n",
       " 0.024993896484375,\n",
       " -0.044891357421875,\n",
       " 0.040069580078125,\n",
       " -0.014190673828125,\n",
       " 0.0278472900390625,\n",
       " -0.002376556396484375,\n",
       " -0.07537841796875,\n",
       " 0.00798797607421875,\n",
       " -0.0235443115234375,\n",
       " -0.04473876953125,\n",
       " -0.0285797119140625,\n",
       " -0.00460052490234375,\n",
       " 0.039794921875,\n",
       " 0.050872802734375,\n",
       " -0.001392364501953125,\n",
       " -0.0880126953125,\n",
       " -0.01389312744140625,\n",
       " 0.009674072265625,\n",
       " -0.04058837890625,\n",
       " 0.004077911376953125,\n",
       " -0.0221405029296875,\n",
       " -0.040374755859375,\n",
       " 0.010284423828125,\n",
       " 0.0760498046875,\n",
       " -0.00848388671875,\n",
       " -0.05328369140625,\n",
       " -0.01517486572265625,\n",
       " -0.028228759765625,\n",
       " -0.01593017578125,\n",
       " 0.041595458984375,\n",
       " -0.021881103515625,\n",
       " 0.01385498046875,\n",
       " -0.056976318359375,\n",
       " 0.0273284912109375,\n",
       " 0.06866455078125,\n",
       " -0.04718017578125,\n",
       " -0.036224365234375,\n",
       " -0.032073974609375,\n",
       " 0.0038318634033203125,\n",
       " -0.005695343017578125,\n",
       " -0.032073974609375,\n",
       " -0.02593994140625,\n",
       " 0.014678955078125,\n",
       " 0.010894775390625,\n",
       " 0.00289154052734375,\n",
       " 0.0110321044921875,\n",
       " 0.00763702392578125,\n",
       " 0.007526397705078125,\n",
       " 0.0238037109375,\n",
       " -0.0105743408203125,\n",
       " 0.01287841796875,\n",
       " 0.00455474853515625,\n",
       " -4.470348358154297e-06,\n",
       " 0.032867431640625,\n",
       " 0.019317626953125,\n",
       " 0.00038814544677734375,\n",
       " 0.053314208984375,\n",
       " -0.02008056640625,\n",
       " 0.0009541511535644531,\n",
       " -0.026336669921875,\n",
       " -0.033233642578125,\n",
       " 0.034088134765625,\n",
       " -0.01244354248046875,\n",
       " 0.0176239013671875,\n",
       " -0.00910186767578125,\n",
       " 0.022064208984375,\n",
       " -0.00896453857421875,\n",
       " -0.046600341796875,\n",
       " -0.04876708984375,\n",
       " -0.0030155181884765625,\n",
       " 0.035247802734375,\n",
       " 0.0161895751953125,\n",
       " 0.045562744140625,\n",
       " -0.0269775390625,\n",
       " 0.0189208984375,\n",
       " -0.026397705078125,\n",
       " 0.04840087890625,\n",
       " -0.01715087890625,\n",
       " -0.005611419677734375,\n",
       " -0.01507568359375,\n",
       " -0.0184326171875,\n",
       " -0.020416259765625,\n",
       " 0.055419921875,\n",
       " 0.03509521484375,\n",
       " 0.006439208984375,\n",
       " 0.0258026123046875,\n",
       " -0.01178741455078125,\n",
       " -0.03253173828125,\n",
       " -0.028594970703125,\n",
       " -0.03125,\n",
       " 0.0035076141357421875,\n",
       " 0.037841796875,\n",
       " 0.005435943603515625,\n",
       " -0.00887298583984375,\n",
       " -0.0241851806640625,\n",
       " -0.0172119140625,\n",
       " 0.0078582763671875,\n",
       " 0.032623291015625,\n",
       " -0.0347900390625,\n",
       " -0.0120391845703125,\n",
       " -0.024993896484375,\n",
       " 0.0129241943359375,\n",
       " 0.03692626953125,\n",
       " -0.060211181640625,\n",
       " 0.0203094482421875,\n",
       " -0.0303497314453125,\n",
       " -0.04730224609375,\n",
       " 0.0082855224609375,\n",
       " 0.0299835205078125,\n",
       " -0.05609130859375,\n",
       " 0.0237884521484375,\n",
       " 0.009979248046875,\n",
       " 0.0248565673828125,\n",
       " -0.01108551025390625,\n",
       " 0.037628173828125,\n",
       " 0.006343841552734375,\n",
       " -0.0238037109375,\n",
       " 0.05450439453125,\n",
       " -0.007049560546875,\n",
       " -0.032684326171875,\n",
       " 0.01181793212890625,\n",
       " 0.016448974609375,\n",
       " 0.0241851806640625,\n",
       " 0.0290985107421875,\n",
       " 0.010650634765625,\n",
       " 0.03717041015625,\n",
       " 0.0196075439453125,\n",
       " -0.057159423828125,\n",
       " 0.0311126708984375,\n",
       " -0.01004791259765625,\n",
       " 0.0254058837890625,\n",
       " 0.0518798828125,\n",
       " -0.006725311279296875,\n",
       " 0.00583648681640625,\n",
       " -0.06646728515625,\n",
       " -0.062744140625,\n",
       " 0.051971435546875,\n",
       " 0.0413818359375,\n",
       " -0.00986480712890625,\n",
       " 0.01158905029296875,\n",
       " 0.037811279296875,\n",
       " 0.0027618408203125,\n",
       " 0.011962890625,\n",
       " -0.0162506103515625,\n",
       " 0.034332275390625,\n",
       " -0.004474639892578125,\n",
       " -0.049591064453125,\n",
       " 0.006626129150390625,\n",
       " 0.009796142578125,\n",
       " 0.033782958984375,\n",
       " -0.014129638671875,\n",
       " -0.01123809814453125,\n",
       " 0.0092010498046875,\n",
       " -0.0116119384765625,\n",
       " -0.0116119384765625,\n",
       " -0.037750244140625,\n",
       " -0.0166015625,\n",
       " -0.0628662109375,\n",
       " -0.0209503173828125,\n",
       " 0.02618408203125,\n",
       " 0.0039215087890625,\n",
       " -0.005126953125,\n",
       " 0.01110076904296875,\n",
       " -0.003173828125,\n",
       " -0.0020236968994140625,\n",
       " 0.0177154541015625,\n",
       " 0.006351470947265625,\n",
       " 0.006145477294921875,\n",
       " -0.071044921875,\n",
       " -0.00994873046875,\n",
       " 0.0116119384765625,\n",
       " 0.04083251953125,\n",
       " -0.00518035888671875,\n",
       " -0.006298065185546875,\n",
       " -0.01107025146484375,\n",
       " -0.06353759765625,\n",
       " -0.00876617431640625,\n",
       " 0.042205810546875,\n",
       " 0.035858154296875,\n",
       " 0.006732940673828125,\n",
       " 0.01849365234375,\n",
       " -0.0298614501953125,\n",
       " -0.00160980224609375,\n",
       " 0.009521484375,\n",
       " 0.005279541015625,\n",
       " -0.05108642578125,\n",
       " 0.0018091201782226562,\n",
       " -0.02593994140625,\n",
       " 0.0268402099609375,\n",
       " -0.051177978515625,\n",
       " 0.027099609375,\n",
       " 0.0038890838623046875,\n",
       " 0.03515625,\n",
       " 0.00689697265625,\n",
       " 0.0192413330078125,\n",
       " 0.0411376953125,\n",
       " -0.01153564453125,\n",
       " -0.017578125,\n",
       " -0.06683349609375,\n",
       " 0.0308074951171875,\n",
       " -0.0008902549743652344,\n",
       " -0.00759124755859375,\n",
       " -0.006221771240234375,\n",
       " 0.0208892822265625,\n",
       " -0.033935546875,\n",
       " 0.0020923614501953125,\n",
       " -0.0213470458984375,\n",
       " -0.01995849609375,\n",
       " -0.0254974365234375,\n",
       " -0.003498077392578125,\n",
       " 0.008331298828125,\n",
       " -0.0021152496337890625,\n",
       " -0.01021575927734375,\n",
       " 0.032958984375,\n",
       " -0.0055389404296875,\n",
       " 0.0183258056640625,\n",
       " 0.0273895263671875,\n",
       " -0.0043182373046875,\n",
       " 0.0051727294921875,\n",
       " -0.009185791015625,\n",
       " 0.0054779052734375,\n",
       " -0.00559234619140625,\n",
       " 0.0286102294921875,\n",
       " 0.018890380859375,\n",
       " 0.01385498046875,\n",
       " 0.04376220703125,\n",
       " 0.0209197998046875,\n",
       " 0.003093719482421875,\n",
       " 0.0010128021240234375,\n",
       " -0.006183624267578125,\n",
       " 0.05059814453125,\n",
       " 0.0308074951171875,\n",
       " -0.0626220703125,\n",
       " -0.01052093505859375,\n",
       " 0.050567626953125,\n",
       " -0.05517578125,\n",
       " -0.024139404296875,\n",
       " 0.0260009765625,\n",
       " -0.056488037109375,\n",
       " 0.003204345703125,\n",
       " 0.0227508544921875,\n",
       " -0.00695037841796875,\n",
       " -0.0251007080078125,\n",
       " -0.00756072998046875,\n",
       " -0.01042938232421875,\n",
       " -0.0187530517578125,\n",
       " 0.042694091796875,\n",
       " -0.02215576171875,\n",
       " 0.029541015625,\n",
       " -0.0014276504516601562,\n",
       " 0.0220184326171875,\n",
       " -0.048065185546875,\n",
       " -0.0102996826171875,\n",
       " 0.012115478515625,\n",
       " 0.0053558349609375,\n",
       " 0.04718017578125,\n",
       " 0.009857177734375,\n",
       " -0.01904296875,\n",
       " 0.00437164306640625,\n",
       " 0.0033435821533203125,\n",
       " 0.0037078857421875,\n",
       " 0.017822265625,\n",
       " 0.044769287109375,\n",
       " -0.019439697265625,\n",
       " -0.04412841796875,\n",
       " -0.03668212890625,\n",
       " 0.0374755859375,\n",
       " -0.01207733154296875,\n",
       " -0.004222869873046875,\n",
       " 0.00846099853515625,\n",
       " 0.04034423828125,\n",
       " -0.047576904296875,\n",
       " 0.039031982421875,\n",
       " 0.029754638671875,\n",
       " -0.006122589111328125,\n",
       " 0.058074951171875,\n",
       " 0.06549072265625,\n",
       " 0.039215087890625,\n",
       " -0.010040283203125,\n",
       " -0.00905609130859375,\n",
       " 0.002155303955078125,\n",
       " 0.04278564453125,\n",
       " -0.056976318359375,\n",
       " -0.02484130859375,\n",
       " 0.0269012451171875,\n",
       " -0.0211029052734375,\n",
       " 0.01226806640625,\n",
       " 0.04766845703125,\n",
       " 0.00368499755859375,\n",
       " 0.026275634765625,\n",
       " -0.037109375,\n",
       " -0.01517486572265625,\n",
       " -0.01314544677734375,\n",
       " -0.02056884765625,\n",
       " 0.00803375244140625,\n",
       " -0.01537322998046875,\n",
       " 0.014678955078125,\n",
       " -0.05096435546875,\n",
       " 0.009185791015625,\n",
       " -0.016815185546875,\n",
       " 0.0260467529296875,\n",
       " -0.0103759765625,\n",
       " 0.0235748291015625,\n",
       " 0.0102081298828125,\n",
       " -0.044830322265625,\n",
       " 0.0433349609375,\n",
       " 0.01364898681640625,\n",
       " 0.0013113021850585938,\n",
       " -0.0115203857421875,\n",
       " -0.007457733154296875,\n",
       " 0.038177490234375,\n",
       " -0.0305023193359375,\n",
       " 0.051666259765625,\n",
       " 0.0106353759765625,\n",
       " 0.01050567626953125,\n",
       " -0.038543701171875,\n",
       " 0.006587982177734375,\n",
       " -0.0203094482421875,\n",
       " -0.03839111328125,\n",
       " 0.0472412109375,\n",
       " 0.017425537109375,\n",
       " 0.06414794921875,\n",
       " -0.0237884521484375,\n",
       " 0.002361297607421875,\n",
       " -4.76837158203125e-07,\n",
       " -0.0081939697265625,\n",
       " 0.0189361572265625,\n",
       " 0.01055908203125,\n",
       " -0.0235748291015625,\n",
       " 0.0233917236328125,\n",
       " -0.053253173828125,\n",
       " -0.033660888671875,\n",
       " 0.038787841796875,\n",
       " 0.01214599609375,\n",
       " 0.0003418922424316406,\n",
       " 0.019927978515625,\n",
       " -0.0024776458740234375,\n",
       " 0.0048980712890625,\n",
       " 0.0011196136474609375,\n",
       " -0.055572509765625,\n",
       " 0.0151519775390625,\n",
       " 0.0227508544921875,\n",
       " -0.05328369140625,\n",
       " -0.03802490234375,\n",
       " 0.0046234130859375,\n",
       " -0.024993896484375,\n",
       " 0.0164794921875,\n",
       " -9.685754776000977e-05,\n",
       " 0.0155181884765625,\n",
       " -0.013519287109375,\n",
       " -0.029388427734375,\n",
       " -0.04119873046875,\n",
       " 0.0556640625,\n",
       " 0.03009033203125,\n",
       " 0.00439453125,\n",
       " -0.01873779296875,\n",
       " -0.004474639892578125,\n",
       " 0.00337982177734375,\n",
       " 0.015472412109375,\n",
       " 0.0234527587890625,\n",
       " 0.01016998291015625,\n",
       " -0.08453369140625,\n",
       " 0.019317626953125,\n",
       " 0.018341064453125,\n",
       " -0.052154541015625,\n",
       " 0.0235443115234375,\n",
       " -0.005428314208984375,\n",
       " -0.02716064453125,\n",
       " 0.0157012939453125,\n",
       " -0.040863037109375,\n",
       " 0.04986572265625,\n",
       " 0.0401611328125,\n",
       " -0.00836181640625,\n",
       " -0.0270843505859375,\n",
       " 0.04339599609375,\n",
       " -0.037841796875,\n",
       " -0.0187530517578125,\n",
       " -0.0254974365234375,\n",
       " 0.036376953125,\n",
       " 0.141357421875,\n",
       " 0.07647705078125,\n",
       " 0.0013151168823242188,\n",
       " -0.062469482421875,\n",
       " -0.00531768798828125,\n",
       " 0.0447998046875,\n",
       " 0.021148681640625,\n",
       " -0.048004150390625,\n",
       " 0.0160369873046875,\n",
       " 0.0202484130859375,\n",
       " -0.02490234375,\n",
       " -4.416704177856445e-05,\n",
       " -0.00494384765625,\n",
       " -0.038482666015625,\n",
       " 0.0248870849609375,\n",
       " -0.018585205078125,\n",
       " -0.0155487060546875,\n",
       " -0.01837158203125,\n",
       " 0.04107666015625,\n",
       " 0.0086822509765625,\n",
       " 0.0183258056640625,\n",
       " -0.01209259033203125,\n",
       " -0.00658416748046875,\n",
       " -0.017364501953125,\n",
       " 0.044403076171875,\n",
       " 0.01727294921875,\n",
       " 0.00015926361083984375,\n",
       " -0.0019178390502929688,\n",
       " 0.01119232177734375,\n",
       " 0.026885986328125,\n",
       " -0.0479736328125,\n",
       " 0.0152435302734375,\n",
       " -0.02117919921875,\n",
       " 0.079345703125,\n",
       " 0.061431884765625,\n",
       " 0.01006317138671875,\n",
       " 0.02069091796875,\n",
       " 0.030548095703125,\n",
       " -0.057403564453125,\n",
       " 0.0273284912109375,\n",
       " -0.01200103759765625,\n",
       " 0.00795745849609375,\n",
       " -0.05206298828125,\n",
       " -0.0032291412353515625,\n",
       " -0.04388427734375,\n",
       " -0.03497314453125,\n",
       " -0.053466796875,\n",
       " -0.0143280029296875,\n",
       " -0.00865936279296875,\n",
       " 0.0088958740234375,\n",
       " 0.06890869140625,\n",
       " 0.0006957054138183594,\n",
       " -0.00958251953125,\n",
       " 0.00420379638671875,\n",
       " -0.0089874267578125,\n",
       " 0.0711669921875,\n",
       " -0.046478271484375,\n",
       " 0.030303955078125,\n",
       " -0.034088134765625,\n",
       " -0.014923095703125,\n",
       " 0.006488800048828125,\n",
       " -0.036163330078125,\n",
       " 0.00940704345703125,\n",
       " -0.01317596435546875,\n",
       " -0.0400390625,\n",
       " 0.009552001953125,\n",
       " -0.040771484375,\n",
       " -0.0099639892578125,\n",
       " 0.0082550048828125,\n",
       " -0.01239776611328125,\n",
       " 0.0247039794921875,\n",
       " -0.02117919921875,\n",
       " -0.043487548828125,\n",
       " -0.0679931640625,\n",
       " 0.0270843505859375,\n",
       " 0.018585205078125,\n",
       " -0.0018444061279296875,\n",
       " -0.043487548828125,\n",
       " -0.02447509765625,\n",
       " -0.04449462890625,\n",
       " -0.00241851806640625,\n",
       " -0.0462646484375,\n",
       " -0.007472991943359375,\n",
       " -0.0252838134765625,\n",
       " -0.04644775390625,\n",
       " -0.0345458984375,\n",
       " 0.032073974609375,\n",
       " 0.032867431640625,\n",
       " 0.0299835205078125,\n",
       " 0.0186614990234375,\n",
       " -0.032196044921875,\n",
       " 0.0240478515625,\n",
       " -0.032501220703125,\n",
       " -0.061004638671875,\n",
       " 0.0079803466796875,\n",
       " 0.0099334716796875,\n",
       " -0.0626220703125,\n",
       " 0.026641845703125,\n",
       " 0.0262298583984375,\n",
       " 0.038330078125,\n",
       " 0.04144287109375,\n",
       " 0.0340576171875,\n",
       " 0.01104736328125,\n",
       " 0.0361328125,\n",
       " -0.02178955078125,\n",
       " 0.0254058837890625,\n",
       " 0.02764892578125,\n",
       " -0.0038394927978515625,\n",
       " -0.01465606689453125,\n",
       " 0.042205810546875,\n",
       " 0.0115966796875,\n",
       " -0.03076171875,\n",
       " -0.01837158203125,\n",
       " -0.007472991943359375,\n",
       " -0.0312042236328125,\n",
       " 0.034393310546875,\n",
       " 0.0261993408203125,\n",
       " -0.053955078125,\n",
       " 0.0177154541015625,\n",
       " 0.048583984375,\n",
       " 0.068115234375,\n",
       " 0.0128021240234375,\n",
       " 0.02069091796875,\n",
       " -0.01319122314453125,\n",
       " 0.019195556640625,\n",
       " -0.038970947265625,\n",
       " -0.056976318359375,\n",
       " 0.006824493408203125,\n",
       " -0.0102691650390625,\n",
       " -0.0160369873046875,\n",
       " -0.016571044921875,\n",
       " 0.0026531219482421875,\n",
       " -0.02734375,\n",
       " -0.0269927978515625,\n",
       " -0.07330322265625,\n",
       " 0.03985595703125,\n",
       " 0.0308074951171875,\n",
       " 0.0474853515625,\n",
       " -0.03594970703125,\n",
       " -0.015716552734375,\n",
       " -0.024444580078125,\n",
       " -0.05657958984375,\n",
       " -0.01226806640625,\n",
       " -0.0298919677734375,\n",
       " -0.0211334228515625,\n",
       " 0.031829833984375,\n",
       " 0.034149169921875,\n",
       " 0.0177154541015625,\n",
       " 0.056182861328125,\n",
       " 0.04937744140625,\n",
       " -0.04168701171875,\n",
       " -0.0199432373046875,\n",
       " 0.00801849365234375,\n",
       " -0.0233917236328125,\n",
       " 0.0229339599609375,\n",
       " 0.0196990966796875,\n",
       " 0.0285797119140625,\n",
       " -0.0178375244140625,\n",
       " 0.01342010498046875,\n",
       " -0.0243988037109375,\n",
       " 0.04150390625,\n",
       " 0.0115203857421875,\n",
       " 0.0095672607421875,\n",
       " 0.0789794921875,\n",
       " -0.0164337158203125,\n",
       " -0.0007572174072265625,\n",
       " 0.0200653076171875,\n",
       " 0.004329681396484375,\n",
       " -0.05731201171875,\n",
       " -0.03216552734375,\n",
       " -0.0266876220703125,\n",
       " 0.060577392578125,\n",
       " 0.0232391357421875,\n",
       " 0.032928466796875,\n",
       " -0.06280517578125,\n",
       " -0.01419830322265625,\n",
       " 0.031280517578125,\n",
       " -0.00940704345703125,\n",
       " 0.04998779296875,\n",
       " 2.7060508728027344e-05,\n",
       " -0.00913238525390625,\n",
       " 0.033843994140625,\n",
       " -0.05316162109375,\n",
       " -0.00278472900390625,\n",
       " -0.052886962890625,\n",
       " 0.0003936290740966797,\n",
       " -0.00583648681640625,\n",
       " -0.0017366409301757812,\n",
       " -0.0306549072265625,\n",
       " 0.01493072509765625,\n",
       " 0.0192718505859375,\n",
       " 0.045745849609375,\n",
       " -0.0137176513671875,\n",
       " -0.0312042236328125,\n",
       " -0.005523681640625,\n",
       " -0.017669677734375,\n",
       " -0.06585693359375,\n",
       " -0.00324249267578125,\n",
       " -0.050201416015625,\n",
       " 0.0075836181640625,\n",
       " 0.02618408203125,\n",
       " -0.06817626953125,\n",
       " 0.01345062255859375,\n",
       " 0.03173828125,\n",
       " 0.015960693359375,\n",
       " 0.0030918121337890625,\n",
       " 0.007801055908203125,\n",
       " 0.062469482421875,\n",
       " 0.04937744140625,\n",
       " 0.045379638671875,\n",
       " 0.04534912109375,\n",
       " -0.0269927978515625,\n",
       " 0.01345062255859375,\n",
       " -0.0120697021484375,\n",
       " -0.042236328125,\n",
       " -0.01126861572265625,\n",
       " -0.004955291748046875,\n",
       " 0.024383544921875,\n",
       " -0.035675048828125,\n",
       " -0.020294189453125,\n",
       " 0.01525115966796875,\n",
       " 0.0206451416015625,\n",
       " -0.02752685546875,\n",
       " -0.0178680419921875,\n",
       " 0.0172271728515625,\n",
       " 0.016357421875,\n",
       " -0.05120849609375,\n",
       " -0.028350830078125,\n",
       " -0.044586181640625,\n",
       " -0.0171356201171875,\n",
       " 0.0281982421875,\n",
       " -0.0234375,\n",
       " 0.0638427734375,\n",
       " -0.040130615234375,\n",
       " -0.0176239013671875,\n",
       " -0.00290679931640625,\n",
       " 0.0019273757934570312,\n",
       " -0.0254364013671875,\n",
       " 0.03643798828125,\n",
       " 0.02349853515625,\n",
       " -0.0305023193359375,\n",
       " -0.0190887451171875,\n",
       " 0.0135955810546875,\n",
       " 0.0175018310546875,\n",
       " -0.016632080078125,\n",
       " -0.053009033203125,\n",
       " -0.0261993408203125,\n",
       " 0.0650634765625,\n",
       " -0.032989501953125,\n",
       " 0.0609130859375,\n",
       " 0.01006317138671875,\n",
       " 0.0278778076171875,\n",
       " 0.007625579833984375,\n",
       " -0.0657958984375,\n",
       " 0.059173583984375,\n",
       " -0.0129241943359375,\n",
       " 0.012176513671875,\n",
       " -0.001621246337890625,\n",
       " 0.007289886474609375,\n",
       " -0.0163116455078125,\n",
       " -0.0032367706298828125,\n",
       " 0.04400634765625,\n",
       " -0.028839111328125,\n",
       " -0.007495880126953125,\n",
       " -0.047698974609375,\n",
       " -0.02294921875,\n",
       " -0.0023784637451171875,\n",
       " -0.0298919677734375,\n",
       " -0.00864410400390625,\n",
       " -0.003955841064453125,\n",
       " -0.0028057098388671875,\n",
       " 0.0006561279296875,\n",
       " -0.05078125,\n",
       " 0.01018524169921875,\n",
       " 0.0273895263671875,\n",
       " 0.0276336669921875,\n",
       " -0.0125732421875,\n",
       " -0.015350341796875,\n",
       " 0.0132293701171875,\n",
       " 0.01213836669921875,\n",
       " 0.003429412841796875,\n",
       " 0.056610107421875,\n",
       " 0.03973388671875,\n",
       " 0.014801025390625,\n",
       " 0.00962066650390625,\n",
       " -0.0347900390625,\n",
       " 0.02508544921875,\n",
       " 0.01898193359375,\n",
       " -0.056610107421875,\n",
       " 0.01457977294921875,\n",
       " 0.00628662109375,\n",
       " 0.0067138671875,\n",
       " 0.0189208984375,\n",
       " 0.07855224609375,\n",
       " 0.017486572265625,\n",
       " 0.000682830810546875,\n",
       " 0.0081024169921875,\n",
       " 0.031646728515625,\n",
       " 0.0064849853515625,\n",
       " 0.004730224609375,\n",
       " 0.03289794921875,\n",
       " 0.0074615478515625,\n",
       " 0.03277587890625,\n",
       " -0.004306793212890625,\n",
       " 0.033660888671875,\n",
       " 0.0125274658203125,\n",
       " 0.026153564453125,\n",
       " -0.0034618377685546875,\n",
       " 0.0286407470703125,\n",
       " 0.047271728515625,\n",
       " -0.0221710205078125,\n",
       " 0.050048828125,\n",
       " 0.0254058837890625,\n",
       " -0.07281494140625,\n",
       " 0.050994873046875,\n",
       " -0.0650634765625,\n",
       " 0.054168701171875,\n",
       " -0.06744384765625,\n",
       " -0.0022411346435546875,\n",
       " 0.0275421142578125,\n",
       " -0.013763427734375,\n",
       " -0.0462646484375,\n",
       " -0.033233642578125,\n",
       " 0.035064697265625,\n",
       " 0.05731201171875,\n",
       " -0.020843505859375,\n",
       " -0.01342010498046875,\n",
       " -0.030242919921875,\n",
       " 0.011871337890625,\n",
       " 0.01409149169921875,\n",
       " 0.01580810546875,\n",
       " 0.004528045654296875,\n",
       " -0.003688812255859375,\n",
       " 0.06463623046875,\n",
       " 0.0095977783203125,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding  \n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embedding_model = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")\n",
    "\n",
    "embedding_model.embed_query(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdfc9d-d6b4-4ce0-8d74-1c232a95dee4",
   "metadata": {},
   "source": [
    "Let's see an example of adding the research paper into the knowledge base so that questions requires the information of the papers can be answered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b91e7-b2a4-4cf8-b8cb-a03afd8dceb2",
   "metadata": {},
   "source": [
    "Before adding the research paper into the knowledge base, the large language model cannot give an answer to question such as \" what is retrieval argmented generation?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e3ce64-68fa-4f05-9722-0a370097390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than two sentences.\"\n",
    "        \"Do not hallucinate. Say you don't know if you don't have this information.\"\n",
    "    )),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4716c79-74b4-45cf-bb6d-8110ae8aeaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" are Sheng Zhang, Xing Niu, and Eric Nyberg.\n"
     ]
    }
   ],
   "source": [
    "# You can replace this question with any question that is specific to the research paper you plan to learn more\n",
    "print(chain.invoke(\"Who are the authors of the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe9e55-5e2c-4979-a9b6-84005297f09a",
   "metadata": {},
   "source": [
    "The answer is not correct. This is because the answer was generated by the large language model which was not trained with the knowledge of this research paper. Although the large language model can handle general questions pretty well, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c946473d-c062-424a-8285-e3d2816c634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A large language model is a type of artificial intelligence (AI) that uses complex algorithms and massive amounts of data to understand and generate human-like language. It's a computer program that can process and respond to natural language inputs, such as text or speech, with a high degree of accuracy and fluency.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"question\": \"What is a large language model\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f0791-c7b7-465a-9318-1cf50bc1d831",
   "metadata": {},
   "source": [
    "To answer the specific question related to this research paper, we can create a knowledge base by adding the research paper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692504eb-6afd-4a67-9eaf-415bff9e5893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4123ed4d-128a-4543-a61c-6d9a595e5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c7a626c-b4cf-4037-a27a-cf832aa52c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than two sentences.\"\n",
    "        \"Do not hallucinate. Say you don't know if you don't have this information.\"\n",
    "        # \"Answer the question using only the context\"\n",
    "        \"\\n\\nQuestion: {question}\\n\\nContext: {context}\"\n",
    "    ),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': vector_store.as_retriever(),\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5588af5c-43c4-4c3a-8890-df455dcc075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" are Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\n"
     ]
    }
   ],
   "source": [
    "# You can replace this question with any question that is specific to the research paper you plan to learn more\n",
    "print(chain.invoke(\"Who are the authors of the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb507b50-eea7-4635-82b6-c28a2fc780dd",
   "metadata": {},
   "source": [
    "Now we have the correct answer which means the LLM was able to retrieve the information of the research paper that was added into the knowledge base. Let's try to ask more questions about the paper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b350e78e-af7e-4fe3-a8ff-75d1cc5501b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is better compared to previous techniques for NLP tasks because it combines pre-trained parametric and non-parametric memory, allowing for more accurate and diverse knowledge retrieval and generation. This approach outperforms previous methods by leveraging the strengths of both parametric and non-parametric memory.\n"
     ]
    }
   ],
   "source": [
    "# You can replace this question with any question that is specific to the research paper you plan to learn more\n",
    "print(chain.invoke(\"Why is Retrieval-Augmented Generation better compared to previous technique for NLP Tasks?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "967f3072-e11a-446b-93ca-f148be646307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The experiment in the paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" was done by fine-tuning a pre-trained sequence-to-sequence transformer model with a non-parametric memory, which is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. The model was trained end-to-end to generate output based on the input and the retrieved documents.\n"
     ]
    }
   ],
   "source": [
    "# You can replace this question with any question that is specific to the research paper you plan to learn more\n",
    "print(chain.invoke(\"How was the experiment done in the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d15a237-1f02-474f-8d5b-84204575dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper reports that the RAG models achieve results within 4.3% of state-of-the-art pipeline models on FEVER fact verification.\n"
     ]
    }
   ],
   "source": [
    "# You can replace this question with any question that is specific to the research paper you plan to learn more\n",
    "print(chain.invoke(\"What is the quantitative experiment result in the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b5107-3e26-4279-9cab-1b65f5003a7e",
   "metadata": {},
   "source": [
    "Finally, let's see how reranking model works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a635ea9-b7eb-47f1-80c2-a6079c545bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve K relevant results to the query\n",
    "query = \"Why is Retrieval-Augmented Generation better compared to previous technique for NLP Tasks?\"\n",
    "retrieved_docs = vector_store.similarity_search(query, k=5, fetch_k=5)\n",
    "len(retrieved_docs)\n",
    "#for doc in retrieved_docs:\n",
    "#    print(doc.page_content)\n",
    "#    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58135032-1902-440f-80b6-c0c3043174fb",
   "metadata": {},
   "source": [
    "After retrieving K results to the query, let reranker calculate the similarity scores, and pick the answer with the highest score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ab35c3f-3216-4838-ade9-8b9842c8b2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'relevance_score': -9.515625}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research;‡University College London;⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem-'),\n",
       " Document(metadata={'relevance_score': -10.875}, page_content='on open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [ 56] fact veriﬁcation, we achieve results within 4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequence xto retrieve text documents zand use them'),\n",
       " Document(metadata={'relevance_score': -12.234375}, page_content='but have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).\\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG'),\n",
       " Document(metadata={'relevance_score': -12.234375}, page_content='basis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG\\ncan be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64,55], stack-\\naugmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.\\nOur results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks —tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform'),\n",
       " Document(metadata={'relevance_score': -13.5859375}, page_content='memory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "query = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "passages = [docs.page_content for docs in retrieved_docs]\n",
    "\n",
    "client = NVIDIARerank(\n",
    "  model=\"nvidia/nv-rerankqa-mistral-4b-v3\", \n",
    "#  top_n=3\n",
    ")\n",
    "\n",
    "response = client.compress_documents(\n",
    "  query=query,\n",
    "  documents=[Document(page_content=passage) for passage in passages]   \n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf065e94-f20b-408a-9b65-22c23370f458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant: Retrieval-Augmented Generation for\n",
      "Knowledge-Intensive NLP Tasks\n",
      "Patrick Lewis†‡, Ethan Perez⋆,\n",
      "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
      "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
      "†Facebook AI Research;‡University College London;⋆New York University;\n",
      "plewis@fb.com\n",
      "Abstract\n",
      "Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
      "stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
      "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
      "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
      "decisions and updating their world knowledge remain open research problems. Pre-\n",
      "trained models with a differentiable access mechanism to explicit non-parametric\n",
      "memory have so far been only investigated for extractive downstream tasks. We\n",
      "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
      "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Most relevant: {response[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3075be57-e928-42df-b03e-411587a534ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least relevant: memory by editing the document index. This approach has also been used in knowledge-intensive\n",
      "dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\n",
      "rather than end-to-end learnt retrieval [9].\n",
      "Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\n",
      "approaches, where a similar training input-output pair is retrieved for a given input, and then edited\n",
      "to provide a ﬁnal output. These approaches have proved successful in a number of domains including\n",
      "Machine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences,\n",
      "including less of emphasis on lightly editing a retrieved item, but on aggregating content from several\n",
      "pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\n",
      "rather than related training pairs. This said, RAG techniques may work well in these settings, and\n",
      "could represent promising future work.\n",
      "6 Discussion\n",
      "In this work, we presented hybrid generation models with access to parametric and non-parametric\n",
      "memory. We showed that our RAG models obtain state of the art results on open-domain QA. We\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Least relevant: {response[-1].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36d1e2-2d62-411f-a0e3-d7bbcaecb8f2",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "NVIDIA: https://build.nvidia.com/explore/discover\n",
    "\n",
    "Langchain: https://python.langchain.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
