{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d2441db-0ad8-40f2-b042-6bb96f184711",
   "metadata": {},
   "source": [
    "# Query Transformations: MultiQueryRetriever\n",
    "\n",
    "Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval. \n",
    "\n",
    "This notebook focuses on multi query retriever, which generate multiple queries based on the original query using LLM, then using each query to retrieve answers.\n",
    "Based on the union of all the answers, a reranking model is used to choose the answer with the highest ranking score.\n",
    "\n",
    "There is also a simple RAG with single query to compare with the multi-query retriever.\n",
    "\n",
    "You don't need to download the models, as the LLM / embedding / reranking models are hosted at NVIDIA endpoints (https://build.nvidia.com/). You need to generate a NVIDIA API key to use the model endpoints.\n",
    "\n",
    "You also need to use Langchain's MultiQueryRetriever. You need a Langchain API key too.\n",
    "\n",
    "\n",
    "## Enviornment: Packages and API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df89be8f-2c49-4f4f-9503-2bff0b08a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install  -qU  nest_asyncio langchain_community tiktoken langchainhub chromadb langchain langchain-nvidia-ai-endpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5258de38-0cc0-4d9d-a5ca-6e750ebe6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'your Langchain API key'\n",
    "# NVIDIA AI Foundation Endpoints\n",
    "os.environ[\"NVIDIA_API_KEY\"] = 'your NVIDIA API key starting with nvapi-'\n",
    "os.environ['USER_AGENT'] = 'myagent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaccdca-1ab0-43b1-82c2-22e9cd27675b",
   "metadata": {},
   "source": [
    "## A Simple RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1b6e2b-dd76-410d-b870-23e02564a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 3/3 [00:00<00:00,  4.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load multiple blogs concurrently: https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/\n",
    "import bs4\n",
    "import nest_asyncio\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "loader = WebBaseLoader([\"https://developer.nvidia.com/blog/autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production/\", \"https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/\", \"https://developer.nvidia.com/blog/getting-kubernetes-ready-for-the-a100-gpu-with-multi-instance-gpu/\"])\n",
    "loader.requests_per_second = 1\n",
    "docs = loader.aload()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8785b29-efef-4ad1-b62e-499b41afc2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding_model=NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"NONE\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4b912e4-1dde-4032-9d2a-9f421b1897c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To autoscale NVIDIA Triton to multiple GPUs using Kubernetes, you can use a Horizontal Pod Autoscaler (HPA) to scale the number of Triton Inference Servers based on the number of inference requests. You can create a PodMonitor to collect NVIDIA Triton metrics and use PromQL to query the metrics from Prometheus. Then, you can define a custom metric in the HPA file to trigger autoscaling.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "llm = ChatNVIDIA(\n",
    "  model=\"meta/llama-3.1-8b-instruct\",\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to autoscale NVIDIA Triton to multiple GPUs using Kubernetes?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f2365b3-b61b-4dbf-ab17-636cbfcaf9e0",
   "metadata": {},
   "source": [
    "## Multi Query Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965de464-0c98-4318-9f9e-f8a597c8d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#follow the doc:https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/MultiQueryRetriever/\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"How to autoscale NVIDIA Triton to multiple GPUs using Kubernetes?\"\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1b6c5-faa9-404b-90c6-22d3b40169fa",
   "metadata": {},
   "source": [
    "### Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f93c9bff-ebfe-419d-a1a0-e0c04fb55948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Here are three alternative versions of the user question to retrieve relevant documents from a vector database:', 'How to configure NVIDIA Triton to automatically scale across multiple GPUs in a Kubernetes environment?', 'What are the best practices for deploying NVIDIA Triton on a Kubernetes cluster with multiple GPUs, and how can I ensure efficient autoscaling?', 'How can I use Kubernetes to dynamically allocate and manage multiple NVIDIA GPUs for NVIDIA Triton inference, and what are the key considerations for autoscaling in this setup?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "retrieved_docs = retriever_from_llm.invoke(question)\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82893604-e70e-4df7-8652-ba26e859bf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'Multi-Instance GPU (MIG) is a new feature of the latest generation of NVIDIA GPUs, such as A100. It enables users to maximize the utilization of a single GPU by…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/getting-kubernetes-ready-for-the-a100-gpu-with-multi-instance-gpu/', 'title': 'Getting Kubernetes Ready for the NVIDIA A100 GPU with Multi-Instance GPU | NVIDIA Technical Blog'}, page_content='\"nvidia.com/gpu.memory\": \"40537\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/gpu.product\": \"A100-SXM4-40GB\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-1g.5gb.count\": \"2\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-1g.5gb.engines.copy\": \"1\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-1g.5gb.engines.decoder\": \"0\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-1g.5gb.engines.encoder\": \"0\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-1g.5gb.engines.jpeg\": \"0\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-1g.5gb.engines.ofa\": \"0\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-1g.5gb.memory\": \"4864\",\\n...\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-3g.20gb.count\": \"2\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-3g.20gb.engines.copy\": \"3\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \"nvidia.com/mig-3g.20gb.engines.decoder\": \"2\",'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='Figure 2. Query the custom metric using PromQL in Prometheus graphical user interface\\nAutoscale Triton Inference Servers'),\n",
       " Document(metadata={'description': 'Learn how to deploy NVIDIA Riva servers on a large scale with Kubernetes for autoscaling and Traefik for load balancing.', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production/', 'title': 'Autoscaling NVIDIA Riva Deployment with Kubernetes for Speech AI in Production | NVIDIA Technical Blog'}, page_content='You can use two NVIDIA Triton metrics to define the custom metric\\xa0 avg_time_queue_ms which means the average queue time per inference request in the past 30 seconds and HPA decides whether to change the replica number based on it.\\n\\nnv_inference_request_success[30] is the number of successful inference requests in the past 30 seconds.\\nnv_inference_queue_duration_us is the cumulative inference queuing duration in microseconds.'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='NAME\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 AGE\\n \\xa0\\xa0\\xa0 kube-prometheus-stack-tritonmetrics\\xa0\\xa0 20s \\nQuery NVIDIA Triton metrics using Prometheus\\nBy default, Prometheus comes with a user interface that can be accessed on port 9090 on the Prometheus server. Open Prometheus in a web browser and choose Status, Targets. You can see that the metrics from three servers are correctly detected by kube-prometheus and added to Prometheus for scrapping.\\nYou can query any NVIDIA Triton metrics such as nv_inference_queue_duration_us\\xa0 or nv_inference_request_success individually or query the following custom metric using PromQL and get the three values calculated by Prometheus (Figure 2). Add avg to get the average value of the three Pods:\\navg(delta(nv_inference_queue_duration_us[30s])/(1+delta(nv_inference_request_success[30s]))). \\nWhen you choose Graph, Prometheus also provides time series data as a graph. We provide more information on this metric in the next section.'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='Figure 4. HPA scales NVIDIA Triton Deployment\\nThe following HPA file flower-hpa.yml autoscales the Deployment of Triton Inference Servers. It uses a Pods metric indicated by the .sepc.metrics field, which takes the average of the given metric across all the Pods controlled by the autoscaling target. The .spec.metrics.targetAverageValue field is specified by considering the value ranges of the custom metric from all the Pods. The field triggers HPA to adjust the number of replicas periodically to match the observed custom metric with the target value.\\n  apiVersion: autoscaling/v2beta1\\n kind: HorizontalPodAutoscaler\\n metadata:\\n \\xa0\\xa0\\xa0 name: flower-hpa\\n spec:\\n \\xa0\\xa0\\xa0 scaleTargetRef:\\n \\xa0\\xa0\\xa0\\xa0 \\xa0apiVersion: apps/v1beta1\\n \\xa0\\xa0\\xa0\\xa0\\xa0 kind: Deployment\\n \\xa0\\xa0\\xa0\\xa0\\xa0 name: flower\\n \\xa0\\xa0\\xa0 minReplicas: 1\\n \\xa0\\xa0\\xa0 maxReplicas: 7\\n \\xa0\\xa0\\xa0 metrics:\\n \\xa0\\xa0\\xa0 - type: Pods\\n \\xa0\\xa0\\xa0\\xa0\\xa0 pods:\\n \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 metricName: avg_time_queue_ms\\n \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 targetAverageValue: 50 \\nCreate the HPA using the command kubectl apply -f flower-hpa.yml and confirm it:\\n $ kubectl get hpa'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='Figure 6. NGINX Plus dashboard showing the number of NVIDIA Triton servers scaled by HPA and each server’s information.\\nConclusion\\nThis post showed the step-by-step instructions and code to deploy Triton Inference Servers at a large scale with MIG in a Kubernetes environment. We also showed you how to autoscale the number of servers and balance the workload using two different types of load balancers. We recorded all the steps and results and you can also watch the Triton Deployment at Scale with Multi-Instance-GPU (MIG) and Kubernetes GTC’21 session.\\nFor more information about using MIG to run multiple deep learning workloads in parallel on a single A100 GPU, see Getting the Most Out of the NVIDIA A100 GPU with MIG.\\n\\n\\nRelated resources\\nDLI course: Deploying a Model for Inference at Production ScaleGTC session: Powering Ad Delivery Systems With AI at Enterprise ScaleGTC session: Deploying, Optimizing, and Benchmarking Large Language Models With Triton Inference ServerNGC Containers: NVIDIA MIG Manager For KubernetesSDK: Triton Management ServiceSDK: NVIDIA Fleet Command\\n\\n\\n\\n\\n Discuss (0)\\n        \\n\\n\\n  \\n\\n      Like    \\n\\n\\n\\n\\nTags'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='In this post, we share the following best practices:\\nDeploying multiple Triton Inference Servers in parallel using MIG on A100Autoscaling the number of Triton Inference Servers based on the number of inference requests using Kubernetes and Prometheus monitoring stack.Using the NGINX Plus load balancer to distribute the inference load evenly among different Triton Inference Servers.\\nThis idea can be applied to multiple A100 or A30 GPUs on a single node or multiple nodes for autoscaling NVIDIA Triton deployment in production. For example, a DGX A100 allows up to 56 Triton Inference Servers (each A100 having up to seven servers using MIG) running on Kubernetes Pods.'),\n",
       " Document(metadata={'description': 'Learn how to deploy NVIDIA Riva servers on a large scale with Kubernetes for autoscaling and Traefik for load balancing.', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production/', 'title': 'Autoscaling NVIDIA Riva Deployment with Kubernetes for Speech AI in Production | NVIDIA Technical Blog'}, page_content='Figure 1. An example of Kubernetes deployment with three replicated Pods, each Pod having a Riva server running on a GPU. Expose NVIDIA Triton metrics to Prometheus using ServiceMonitor and kube-prometheus.\\nIn order to automatically change the number of Riva servers on Kubernetes Pods, you need a ServiceMonitor to monitor Riva Service for target discovery by Prometheus. You also need kube-Prometheus to deploy Prometheus and link Prometheus to metric endpoints. In the yaml file below, ServiceMonitor is set to monitor Riva Service every 15 seconds.\\napiVersion: monitoring.coreos.com/v1\\nkind: ServiceMonitor\\nmetadata:  \\n     name: {{ template \"riva-server-metrics-monitor.fullname\" . }}  \\n     namespace: {{ .Release.Namespace }}  \\n     labels:    \\n        app: {{ template \"riva-server-metrics-monitor.name\" . }}    \\n        chart: {{ template \"riva-server.chart\" . }}   \\n        release: {{ .Release.Name }}    \\nspec:  \\n    selector:'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='In the Kubernetes environment, you must install the NVIDIA device plug-in and GPU feature discovery plug-in to be able to use MIG. You could install each plug-in separately, or use the cloud-native NVIDIA GPU Operator, which is a single package that includes everything to enable GPU in Kubernetes. You can also use the NVIDIA deployment tool DeepOps, which takes care of installation and plug-ins, and the Prometheus monitoring stack including kube-prometheus, Prometheus, and the Prometheus adapter, that you should use for autoscaling Triton Inference Servers.\\nYou could use either single strategy or mixed strategy for MIG in Kubernetes. In this post, we suggest the mixed strategy, as you have seven MIG devices for one A100 GPU while the other A100 MIG is disabled.\\nUse the Flower demo, which classifies the images of flowers using ResNet50. The NVIDIA Triton Inference Server container image can be pulled from NGC. Prepare the server’s model files (*.plan, config.pbtxt) and client for the Flower demo. For more information, see Minimizing Deep Learning Inference Latency with NVIDIA Multi-Instance GPU.\\nFlower demo with Kubernetes'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='Figure 1. (left) Clients sending inference requests to Triton Inference Servers running on MIG devices in Kubernetes. (right) The client getting classification results and performance numbers.\\nSo far, you have multiple Triton Inference Servers running on MIG devices in Kubernetes environments, doing inference on the flower images sent by the client and you can manually change the number of servers. In the next sections, you improve it so that the number of servers can be autoscaled based on the client requests.\\nUse Prometheus to scrape NVIDIA Triton metrics\\nTo automatically change the number of Triton Inference servers running on Kubernetes Pods, first collect NVIDIA Triton metrics that can be used to define a custom metric. Because there are several sets of NVIDIA Triton metrics from multiple Kubernetes Pods, you should deploy a PodMonitor that tells Prometheus to scrape the metrics from all the Pods.\\nPrometheus is an open-source, systems monitoring and alerting toolkit that provides time series data identified by metric name and key/value pairs. PromQL, a flexible query language, is used to query metrics from Prometheus.\\nCreate PodMonitor for Prometheus'),\n",
       " Document(metadata={'description': 'NVIDIA Triton can manage any number and mix of models, support multiple deep-learning frameworks, and integrate easily with Kubernetes for large-scale…', 'language': 'en-US', 'source': 'https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/', 'title': 'Deploying NVIDIA Triton at Scale with MIG and Kubernetes | NVIDIA Technical Blog'}, page_content='Flower demo with Kubernetes\\nAfter setting up the flower demo, you want to extend it to a Deployment in a Kubernetes environment. Do this so that the number of Triton Inference Servers can be autoscaled based on the inference requests and the inference load can be distributed among all the servers. Because it allows up to seven MIG devices on an A100, you can have up to seven Kubernetes Pods, each having a Triton Inference Server running on a MIG device. Here are the major steps to deploying Triton Inference Servers with autoscaling and load balancing:\\nCreate a Kubernetes Deployment for Triton Inference Servers.Create a Kubernetes Service to expose Triton Inference Servers as a network service.Expose NVIDIA Triton metrics to Prometheus using kube-prometheus and PodMonitor.Create ConfigMap to define a custom metric.Deploy Prometheus Adapter and expose the custom metric as a registered Kubernetes APIService.Create HPA (Horizontal Pod Autoscaler) to use the custom metric.Use NGINX Plus load balancer to distribute inference requests among all the Triton Inference servers.\\nThe following sections provide the step-by-step guide to achieve these goals.\\nCreate a Kubernetes Deployment for Triton Inference Servers')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "618afe4a-f1d6-433a-9d28-f1956c2b83ef",
   "metadata": {},
   "source": [
    "## Reranker\n",
    "After retrieving multiple results to multiple queries, let reranker calculate the similarity scores, and pick the answer with the highest score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5898291-5e95-40f9-9033-3ee5f298dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "query = \"How to autoscale NVIDIA Triton to multiple GPUs using Kubernetes?\"\n",
    "\n",
    "passages = [docs.page_content for docs in retrieved_docs]\n",
    "\n",
    "client = NVIDIARerank(\n",
    "  model=\"nvidia/nv-rerankqa-mistral-4b-v3\", \n",
    "  truncate=\"END\",\n",
    "  top_n=5\n",
    ")\n",
    "\n",
    "response = client.compress_documents(\n",
    "  query=query,\n",
    "  documents=[Document(page_content=passage) for passage in passages]   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb5e10b-db01-4a85-8f4b-8da6640d25dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant: In this post, we share the following best practices:\n",
      "Deploying multiple Triton Inference Servers in parallel using MIG on A100Autoscaling the number of Triton Inference Servers based on the number of inference requests using Kubernetes and Prometheus monitoring stack.Using the NGINX Plus load balancer to distribute the inference load evenly among different Triton Inference Servers.\n",
      "This idea can be applied to multiple A100 or A30 GPUs on a single node or multiple nodes for autoscaling NVIDIA Triton deployment in production. For example, a DGX A100 allows up to 56 Triton Inference Servers (each A100 having up to seven servers using MIG) running on Kubernetes Pods.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Most relevant: {response[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f0fa71-fea7-4637-9a5b-587cf4c5fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Least relevant: {response[-1].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69021f23-03f6-4979-993c-7c214a9f9304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'relevance_score': 27.171875}, page_content='In this post, we share the following best practices:\\nDeploying multiple Triton Inference Servers in parallel using MIG on A100Autoscaling the number of Triton Inference Servers based on the number of inference requests using Kubernetes and Prometheus monitoring stack.Using the NGINX Plus load balancer to distribute the inference load evenly among different Triton Inference Servers.\\nThis idea can be applied to multiple A100 or A30 GPUs on a single node or multiple nodes for autoscaling NVIDIA Triton deployment in production. For example, a DGX A100 allows up to 56 Triton Inference Servers (each A100 having up to seven servers using MIG) running on Kubernetes Pods.'),\n",
       " Document(metadata={'relevance_score': 17.671875}, page_content='Figure 1. (left) Clients sending inference requests to Triton Inference Servers running on MIG devices in Kubernetes. (right) The client getting classification results and performance numbers.\\nSo far, you have multiple Triton Inference Servers running on MIG devices in Kubernetes environments, doing inference on the flower images sent by the client and you can manually change the number of servers. In the next sections, you improve it so that the number of servers can be autoscaled based on the client requests.\\nUse Prometheus to scrape NVIDIA Triton metrics\\nTo automatically change the number of Triton Inference servers running on Kubernetes Pods, first collect NVIDIA Triton metrics that can be used to define a custom metric. Because there are several sets of NVIDIA Triton metrics from multiple Kubernetes Pods, you should deploy a PodMonitor that tells Prometheus to scrape the metrics from all the Pods.\\nPrometheus is an open-source, systems monitoring and alerting toolkit that provides time series data identified by metric name and key/value pairs. PromQL, a flexible query language, is used to query metrics from Prometheus.\\nCreate PodMonitor for Prometheus'),\n",
       " Document(metadata={'relevance_score': 17.671875}, page_content='Figure 6. NGINX Plus dashboard showing the number of NVIDIA Triton servers scaled by HPA and each server’s information.\\nConclusion\\nThis post showed the step-by-step instructions and code to deploy Triton Inference Servers at a large scale with MIG in a Kubernetes environment. We also showed you how to autoscale the number of servers and balance the workload using two different types of load balancers. We recorded all the steps and results and you can also watch the Triton Deployment at Scale with Multi-Instance-GPU (MIG) and Kubernetes GTC’21 session.\\nFor more information about using MIG to run multiple deep learning workloads in parallel on a single A100 GPU, see Getting the Most Out of the NVIDIA A100 GPU with MIG.\\n\\n\\nRelated resources\\nDLI course: Deploying a Model for Inference at Production ScaleGTC session: Powering Ad Delivery Systems With AI at Enterprise ScaleGTC session: Deploying, Optimizing, and Benchmarking Large Language Models With Triton Inference ServerNGC Containers: NVIDIA MIG Manager For KubernetesSDK: Triton Management ServiceSDK: NVIDIA Fleet Command\\n\\n\\n\\n\\n Discuss (0)\\n        \\n\\n\\n  \\n\\n      Like    \\n\\n\\n\\n\\nTags'),\n",
       " Document(metadata={'relevance_score': 16.3125}, page_content='Flower demo with Kubernetes\\nAfter setting up the flower demo, you want to extend it to a Deployment in a Kubernetes environment. Do this so that the number of Triton Inference Servers can be autoscaled based on the inference requests and the inference load can be distributed among all the servers. Because it allows up to seven MIG devices on an A100, you can have up to seven Kubernetes Pods, each having a Triton Inference Server running on a MIG device. Here are the major steps to deploying Triton Inference Servers with autoscaling and load balancing:\\nCreate a Kubernetes Deployment for Triton Inference Servers.Create a Kubernetes Service to expose Triton Inference Servers as a network service.Expose NVIDIA Triton metrics to Prometheus using kube-prometheus and PodMonitor.Create ConfigMap to define a custom metric.Deploy Prometheus Adapter and expose the custom metric as a registered Kubernetes APIService.Create HPA (Horizontal Pod Autoscaler) to use the custom metric.Use NGINX Plus load balancer to distribute inference requests among all the Triton Inference servers.\\nThe following sections provide the step-by-step guide to achieve these goals.\\nCreate a Kubernetes Deployment for Triton Inference Servers'),\n",
       " Document(metadata={'relevance_score': 14.9453125}, page_content='In the Kubernetes environment, you must install the NVIDIA device plug-in and GPU feature discovery plug-in to be able to use MIG. You could install each plug-in separately, or use the cloud-native NVIDIA GPU Operator, which is a single package that includes everything to enable GPU in Kubernetes. You can also use the NVIDIA deployment tool DeepOps, which takes care of installation and plug-ins, and the Prometheus monitoring stack including kube-prometheus, Prometheus, and the Prometheus adapter, that you should use for autoscaling Triton Inference Servers.\\nYou could use either single strategy or mixed strategy for MIG in Kubernetes. In this post, we suggest the mixed strategy, as you have seven MIG devices for one A100 GPU while the other A100 MIG is disabled.\\nUse the Flower demo, which classifies the images of flowers using ResNet50. The NVIDIA Triton Inference Server container image can be pulled from NGC. Prepare the server’s model files (*.plan, config.pbtxt) and client for the Flower demo. For more information, see Minimizing Deep Learning Inference Latency with NVIDIA Multi-Instance GPU.\\nFlower demo with Kubernetes')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08bb78-7e80-4d95-a124-33b695bf5e6a",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "\n",
    "NVIDIA: https://build.nvidia.com/explore/discover\n",
    "\n",
    "Langchain: https://github.com/langchain-ai/rag-from-scratch/tree/main\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever\n",
    "\n",
    "LangSmith: https://docs.smith.langchain.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
